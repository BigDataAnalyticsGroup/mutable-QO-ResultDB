{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9442ef-9de6-4173-bb6c-2b5b6b3ecea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from natsort import natsort_keygen, natsorted # to naturally sort string columns\n",
    "\n",
    "plt.style.use('matplotlibrc')\n",
    "\n",
    "KB = 1024\n",
    "MB = 1024 * 1024\n",
    "GB = 1024 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d797e-6378-46c6-9f62-a6f4e823bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_size(fraction_width=0.95, fraction_height=0.25):\n",
    "    width_pt = 241.14749 # column width in pt\n",
    "    height_pt = 626.0 # page height in pt\n",
    "\n",
    "    fig_width_pt = width_pt * fraction_width\n",
    "    fig_height_pt = height_pt * fraction_height\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    fig_height_in = fig_height_pt * inches_per_pt\n",
    "\n",
    "    return (fig_width_in, fig_height_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d0d46-0cbc-4bd8-bc33-d0e678318289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define color to use throughout the notebook\n",
    "paired = matplotlib.colormaps['Paired']\n",
    "tab10 = matplotlib.colormaps['tab20']\n",
    "dark2 = matplotlib.colormaps['Dark2']\n",
    "\n",
    "c_st =  tab10(0)\n",
    "c_decomp = tab10(1)\n",
    "c_rdb_w_pj = tab10(2)\n",
    "c_rdb_wo_pj = tab10(4)\n",
    "c_redundancy = tab10(14)\n",
    "\n",
    "c_rm1 = dark2(0)\n",
    "c_rm2 = dark2(2)\n",
    "c_rm3 = dark2(5)\n",
    "c_rm4 = dark2(3)\n",
    "\n",
    "colormap = {\n",
    "    'Single Table': c_st,\n",
    "    'Decompose': c_decomp,\n",
    "    'RDB w/ post-join info': c_rdb_w_pj,\n",
    "    'Result DB': c_rdb_w_pj,\n",
    "    'RDB w/o post-join info': c_rdb_wo_pj,\n",
    "    '0. Single Table': c_st,\n",
    "    'RM1. Dynamic SELECT DISTINCT': c_rm1,\n",
    "    'RM2. Materialized SELECT DISTINCT': c_rm2,\n",
    "    'RM3. Dynamic Subquery': c_rm3,\n",
    "    'RM4. Materialized Subquery': c_rm4,\n",
    "    'Redundancy': c_redundancy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf7022-d5d7-41ad-9d99-da372f253fe8",
   "metadata": {},
   "source": [
    "# Result Set Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8e0d8-b925-48a7-a3ab-c5f6db51a027",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43904c8-eab9-4591-81c1-32befb03cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compression_ratio(uncompressed, compressed):\n",
    "    \"\"\"\n",
    "    Compute the compression ratio.\n",
    "\n",
    "    Returns:\n",
    "    float: Compression ratio.\n",
    "    \"\"\"\n",
    "    # assert uncompressed != 0, \"uncompressed must not be zero\"\n",
    "    if compressed == 0:\n",
    "        return 0\n",
    "    return round(uncompressed / compressed, 2)\n",
    "\n",
    "data = pd.read_csv('../result-set-sizes/result-set-sizes.csv')\n",
    "data = data.drop(['relation', 'count'], axis=1)\n",
    "data = data.groupby(by=['database', 'query', 'method'], as_index=False).sum()\n",
    "data = data.sort_values(by=['query', 'method'], key=natsort_keygen())\n",
    "data['size'] = round(data['size'] / KB, 2)\n",
    "\n",
    "single_table = data[data['method'] == 'Single Table']\n",
    "rdb_w_post_join_info = data[data['method'] == 'rdb_w_post_join_info']\n",
    "rdb_w_post_join_info['compression_ratio'] = [ compression_ratio(baseline, new) for baseline, new in zip(single_table['size'], rdb_w_post_join_info['size']) ]\n",
    "rdb_wo_post_join_info = data[data['method'] == 'rdb_wo_post_join_info']\n",
    "rdb_wo_post_join_info['compression_ratio'] = [ compression_ratio(baseline, new) for baseline, new in zip(single_table['size'], rdb_wo_post_join_info['size']) ]\n",
    "\n",
    "display(single_table)\n",
    "display(rdb_w_post_join_info)\n",
    "display(rdb_wo_post_join_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da871a-b636-49ed-80a0-2e2bbcd5008c",
   "metadata": {},
   "source": [
    "## Synthetic\n",
    "\n",
    "Star-schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72ff49-ee4f-4fc2-8ff3-9e2fcfa85551",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim_tables = 4 # number of dimension tables\n",
    "dim_payload = 20 # byte\n",
    "dim_row_size = 4 + dim_payload # 4 byte primary key + payload\n",
    "\n",
    "fact_payload = 20\n",
    "fact_row_size = fact_payload + num_dim_tables * 4 # 4 byte foreign key per dimension table + payload\n",
    "\n",
    "dim1_size = 60\n",
    "dim2_size = 60\n",
    "dim3_size = 60\n",
    "dim4_size = 60\n",
    "fact_size = dim1_size * dim2_size * dim3_size * dim4_size\n",
    "\n",
    "fact_size_selectivities = np.arange(0.1, 1.1, 0.1)\n",
    "fact_sizes = [ fact_size * sel for sel in fact_size_selectivities ]\n",
    "\n",
    "single_table = [ ((fact_row_size + dim_row_size * num_dim_tables) * f_size) / (1024**2) for f_size in fact_sizes ]\n",
    "rdb_with_post_join = [ (dim_row_size * (dim1_size + dim2_size + dim3_size + dim4_size) + fact_row_size * f_size) / (1024**2) for f_size in fact_sizes ]\n",
    "rdb_without_post_join = [ (dim_row_size * (dim1_size + dim2_size + dim3_size + dim4_size)) / (1024**2) for _ in fact_sizes ]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "\n",
    "plt.plot(fact_sizes, single_table, 'o--', color=colormap['Single Table'], label='Single Table')\n",
    "plt.plot(fact_sizes, rdb_with_post_join, 'x--', color=colormap['RDB w/ post-join info'], label='RDB w/ post-join info')\n",
    "plt.plot(fact_sizes, rdb_without_post_join, '^--', color=colormap['RDB w/o post-join info'], label='RDB w/o post-join info')\n",
    "\n",
    "\n",
    "ax.set_xlabel('\\#tuples in Fact table')\n",
    "ax.set_ylabel('Size [MiB]')\n",
    "ax.set_title('Synthetic Result Set Sizes')\n",
    "# ax.set_xticks(x + width, queries)\n",
    "ax.legend(loc='upper left', ncols=1)\n",
    "# ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "fig.savefig('synthetic-result-sizes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc7163-ee30-4a32-a5f8-fa8471812c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim_tables = 4 # number of dimension tables\n",
    "dim_payload = 20 # byte\n",
    "dim_row_size = 4 + dim_payload # 4 byte primary key + payload\n",
    "dim_table_size = 60\n",
    "selectivities = np.arange(0.1, 1.1, 0.1)\n",
    "dim_table_sizes = [ int(dim_table_size * sel) for sel in selectivities ]\n",
    "\n",
    "fact_payload = 20\n",
    "fact_row_size = 4 + num_dim_tables * 4 + fact_payload # 4 byte pk + 4 byte foreign key per dimension table + payload\n",
    "fact_size = dim_table_size**num_dim_tables\n",
    "fact_table_sizes = [ dim_size**num_dim_tables for dim_size in dim_table_sizes ]\n",
    "\n",
    "single_table = [ ((fact_row_size + dim_row_size * num_dim_tables) * f_size) / MB for f_size in fact_table_sizes ]\n",
    "rdb_with_post_join = ([ ((f_size * fact_row_size) + (dim_row_size * d_size * num_dim_tables)) / MB\n",
    "                       for f_size, d_size in zip(fact_table_sizes, dim_table_sizes) ])\n",
    "rdb_without_post_join = [ (dim_row_size * d_size * num_dim_tables) / MB for d_size in dim_table_sizes ]\n",
    "\n",
    "### Plot\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(3,2))\n",
    "# display(*single_table/rdb_with_post_join)\n",
    "# for s, r in zip(single_table, rdb_with_post_join):\n",
    "#     display(s/r)\n",
    "# display(rdb_with_post_join)\n",
    "plt.plot(selectivities, single_table, 'o--', color=colormap['Single Table'], label='Single Table')\n",
    "plt.plot(selectivities, rdb_with_post_join, 'x--', color=colormap['RDB w/ post-join info'], label='RDB w/ post-join info')\n",
    "plt.plot(selectivities, rdb_without_post_join, '^--', color=colormap['RDB w/o post-join info'], label='RDB w/o post-join info')\n",
    "\n",
    "ax.set_xlabel('Dimension table selectivities | redundancy')\n",
    "ax.set_ylabel('Size [MiB]')\n",
    "# ax.set_title('Synthetic Result Set Sizes')\n",
    "# ax.set_xticks(selectivities)\n",
    "ax.legend(loc='upper left', ncols=1)\n",
    "# ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "fig.savefig('synthetic-result-sizes.pdf')\n",
    "\n",
    "# TODO: make a plot that varies (x-axis) the redundancy or shows (y-axis) the redundancy for different selectivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43010f28-fb1c-4bd2-a95a-816a823906d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim_tables = 4 # number of dimension tables\n",
    "dim_payload = 20 # byte\n",
    "dim_row_size = 4 + dim_payload # 4 byte primary key + payload\n",
    "dim_table_size = 60\n",
    "selectivities = np.arange(0.1, 1.1, 0.1)\n",
    "dim_table_sizes = [ int(dim_table_size * sel) for sel in selectivities ]\n",
    "\n",
    "fact_payload = 20\n",
    "fact_row_size = 4 + num_dim_tables * 4 + fact_payload # 4 byte pk + 4 byte foreign key per dimension table + payload\n",
    "fact_size = dim_table_size**num_dim_tables\n",
    "fact_table_sizes = [ dim_size**num_dim_tables for dim_size in dim_table_sizes ]\n",
    "\n",
    "single_table = [ ((fact_row_size + dim_row_size * num_dim_tables) * f_size) / MB for f_size in fact_table_sizes ]\n",
    "rdb_with_post_join = ([ ((f_size * fact_row_size) + (dim_row_size * d_size * num_dim_tables)) / MB\n",
    "                       for f_size, d_size in zip(fact_table_sizes, dim_table_sizes) ])\n",
    "rdb_without_post_join = [ (dim_payload * d_size * num_dim_tables + f_size * (fact_payload)) / MB for f_size, d_size in zip(fact_table_sizes, dim_table_sizes) ]\n",
    "\n",
    "print(single_table)\n",
    "print(rdb_with_post_join)\n",
    "### Plot\n",
    "fig, ax = plt.subplots(figsize=set_size(fraction_width=1.0, fraction_height=0.22), layout='constrained')\n",
    "# display(*single_table/rdb_with_post_join)\n",
    "# for s, r in zip(single_table, rdb_with_post_join):\n",
    "#     display(s/r)\n",
    "display(rdb_with_post_join)\n",
    "ax.plot(selectivities, single_table, 'o--', color=colormap['Single Table'], label='Single Table')\n",
    "ax.plot(selectivities, rdb_with_post_join, 'x--', color=colormap['RDB w/ post-join info'], label='RDB w/ post-join info')\n",
    "ax.plot(selectivities, rdb_without_post_join, '^--', color=colormap['RDB w/o post-join info'], label='RDB w/o post-join info')\n",
    "\n",
    "ax.fill_between(selectivities, single_table, rdb_with_post_join, color=colormap['Redundancy'], label='Redundancy')\n",
    "\n",
    "ax.set_xlabel('Selectivity')\n",
    "ax.set_ylabel('Result Set Size [MiB]')\n",
    "# ax.set_title('Synthetic Result Set Sizes')\n",
    "# ax.set_xticks(selectivities)\n",
    "# fig.legend(ncols=int(len(method_times)/1), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "ax.legend(loc='upper left', ncols=1)\n",
    "# ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "fig.savefig('synthetic-result-sizes.pdf', bbox_inches='tight')\n",
    "\n",
    "# TODO: make a plot that varies (x-axis) the redundancy or shows (y-axis) the redundancy for different selectivities\n",
    "# showing the redundancy does not really work well because both RDB approaches do not incur any redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1889e-82f1-49a5-ad15-a073c6a16c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim_tables = 4 # number of dimension tables\n",
    "dim_table_size = 60\n",
    "\n",
    "dim_payloads = [0, 50, 100, 150, 200]\n",
    "\n",
    "fact_payload = 20\n",
    "fact_row_size = 4 + fact_payload + num_dim_tables * 4 # 4 byte pk + payload + 4 byte foreign key per dimension table\n",
    "fact_size = dim_table_size**num_dim_tables\n",
    "\n",
    "single_table = []\n",
    "rdb_with_post_join = []\n",
    "rdb_without_post_join = []\n",
    "\n",
    "for dim_payload in dim_payloads:\n",
    "    dim_row_size = 4 + dim_payload\n",
    "    single_table_row_size = fact_row_size + num_dim_tables * dim_row_size\n",
    "    single_table.append((single_table_row_size * fact_size) / MB)\n",
    "    rdb_with_post_join.append((fact_row_size * fact_size + (num_dim_tables * dim_row_size * dim_table_size)) / MB)\n",
    "    rdb_without_post_join.append((num_dim_tables * dim_row_size * dim_table_size) / MB)\n",
    "\n",
    "# single_table = [ ((fact_row_size + (4 + dim_payload) * dim_table_size * num_dim_tables) * fact_size) / MB for dim_payload in dim_payloads ]\n",
    "# rdb_with_post_join = ([ (fact_size * fact_row_size + (4 + dim_payload) * dim_table_size * num_dim_tables) / MB for dim_payload in dim_payloads ])\n",
    "# rdb_without_post_join = [ ((4 + dim_payload) * dim_table_size * num_dim_tables) / MB for dim_payload in dim_payloads ]\n",
    "\n",
    "# display(single_table)\n",
    "# display(rdb_with_post_join)\n",
    "### Plot\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(3,2))\n",
    "# display(*single_table/rdb_with_post_join)\n",
    "for s, r in zip(single_table, rdb_with_post_join):\n",
    "    display(s/r)\n",
    "\n",
    "# display(rdb_with_post_join)\n",
    "plt.plot(dim_payloads, single_table, 'o--', color=colormap['Single Table'], label='Single Table')\n",
    "plt.plot(dim_payloads, rdb_with_post_join, 'x--', color=colormap['RDB w/ post-join info'], label='RDB w/ post-join info')\n",
    "plt.plot(dim_payloads, rdb_without_post_join, '^--', color=colormap['RDB w/o post-join info'], label='RDB w/o post-join info')\n",
    "\n",
    "ax.set_xlabel('payload of each dimension table [bytes]')\n",
    "ax.set_ylabel('Size [MiB]')\n",
    "# ax.set_title('Synthetic Result Set Sizes')\n",
    "# ax.set_xticks(x + width, queries)\n",
    "ax.legend(loc='upper left', ncols=1)\n",
    "# ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "fig.savefig('synthetic-result-sizes-payload.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a5204-3b80-4314-b870-41dc52f59685",
   "metadata": {},
   "source": [
    "#### Memory Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e21ba-95c7-4d47-8e3e-827b9cc7b850",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d520c-ce30-4d2a-885d-f196b033dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./memory-consumption.csv')\n",
    "data = data.sort_values(by=['benchmark', 'query'], key=natsort_keygen())\n",
    "queries = data['query'].unique()\n",
    "method_sizes = {\n",
    "    \"Single Table\": [],\n",
    "    \"Result DB\": []\n",
    "}\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    benchmark = row['benchmark']\n",
    "    query = row['query']\n",
    "    resultdb = row['resultdb']\n",
    "    size = row['size_MiB']\n",
    "    if (resultdb == 0):\n",
    "        method_sizes['Single Table'].append(size)\n",
    "    else:\n",
    "        method_sizes['Result DB'].append(size)\n",
    "\n",
    "x = np.arange(len(queries))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "for m, size in method_sizes.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, size, width, color=colormap[m], label=m)\n",
    "    # ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_ylabel('Size [MiB] (log-scale)')\n",
    "ax.set_title('JOB Memory Consumption')\n",
    "ax.set_xticks(x + width/2, queries)\n",
    "ax.legend(loc='upper right', ncols=1)\n",
    "# ax.set_ylim(0)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "fig.savefig('job-memory-consumption.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ebb92-9d68-4c8e-9a92-7f222c29dee4",
   "metadata": {},
   "source": [
    "# Rewrite Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d023a-fae6-4a3a-9760-6b72b93469b8",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4da4c-0147-460c-882d-347edcc26918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # construct the following dictionary:\n",
    "    # {\n",
    "    #   'RM0': [q0_time, q1_time, ..., qn-1_time]\n",
    "    #   'RM1': [q0_time, q1_time, ..., qn-1_time]\n",
    "    # }\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    method_times = defaultdict(list)\n",
    "    for _, row in data.iterrows():\n",
    "        method_times[row['method']].append(row['time'])\n",
    "    return method_times\n",
    "\n",
    "def bar_plot(data, data_transfer, filename):\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    method_times = preprocess_data(data[data['data_transfer'] == data_transfer])\n",
    "    x = np.arange(len(queries))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    multiplier = 0\n",
    "    fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.26), layout='constrained')\n",
    "    for method, times in method_times.items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, times, width, color=colormap[method], label=method)\n",
    "        # ax.bar_label(rects, padding=3, size=8)\n",
    "        multiplier += 1\n",
    "    ax.set_xticks(x + width + width/2, queries)\n",
    "    ax.set_xlabel('JOB Queries')\n",
    "    ax.legend(loc='upper left')\n",
    "    # fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "    ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "    \n",
    "    log_scale = True\n",
    "    if log_scale:\n",
    "        ax.set_ylabel('Query Execution Time [ms] (log-scale)')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_ylabel('Query Execution Time [ms]')\n",
    "        ax.set_ylim(0)\n",
    "        \n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "data = pd.read_csv('../rewrite-methods/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "exclude_queries = [\n",
    "    # 'q13a',\n",
    "    # 'q17a'\n",
    "]\n",
    "data = data[~data['query'].isin(exclude_queries)]\n",
    "exclude_methods = [\n",
    "    '0. Single Table'\n",
    "]\n",
    "data = data[~data['method'].isin(exclude_methods)]\n",
    "\n",
    "bar_plot(data, False, 'job-rewrite-methods.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ae1f3-1ba6-48fb-8338-e2fdef97b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overhead(baseline, new):\n",
    "    assert baseline != 0, \"baseline must not be zero\"\n",
    "    if (new <= baseline): # new is faster -> improvement\n",
    "        return -((baseline - new) / baseline) * 100\n",
    "    else: # new is slower -> overhead \n",
    "        return ((new - baseline) / baseline) * 100\n",
    "        \n",
    "data = pd.read_csv('../rewrite-methods/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "data = data[data['data_transfer'] == False]\n",
    "exclude_queries = [\n",
    "    # 'q13a',\n",
    "]\n",
    "data = data[~data['query'].isin(exclude_queries)]\n",
    "# methods = data['method'].unique()[1:]\n",
    "\n",
    "single_table = data[data['method'] == '0. Single Table']\n",
    "rewrite_methods = data[data['method'] != '0. Single Table']\n",
    "\n",
    "# NOTE: be careful as ties are included (handle them manually)\n",
    "min_values = rewrite_methods.groupby(by=['query'])['time'].transform('min')\n",
    "best_rewrite_method = rewrite_methods[rewrite_methods['time'] == min_values]\n",
    "\n",
    "# ensure that both dataframes are sorted correctly\n",
    "single_table = single_table.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "best_rewrite_method = best_rewrite_method.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "\n",
    "display(single_table)\n",
    "display(best_rewrite_method)\n",
    "\n",
    "assert len(single_table) == len(best_rewrite_method)\n",
    "\n",
    "for (index1, st), (index2, rm) in zip(single_table.iterrows(), best_rewrite_method.iterrows()):\n",
    "    print(f\"{st['query']}: {round(overhead(int(st['time']), int(rm['time'])), 1)}\")\n",
    "    # print(f\"{st['query']}: {round(int(st['time']) / int(rm['time']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4635ee-63e4-4f90-abe9-bf92b7df5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_improvement_factor(baseline, new):\n",
    "    \"\"\"\n",
    "    Compute the improvement factor of a new method against a baseline.\n",
    "\n",
    "    Args:\n",
    "    baseline (float): Performance metric of the baseline method.\n",
    "    new (float): Performance metric of the new method.\n",
    "\n",
    "    Returns:\n",
    "    float: Improvement factor as a percentage.\n",
    "    \"\"\"\n",
    "    assert baseline != 0, \"baseline must not be zero\"\n",
    "    # return ((baseline - new) / baseline) * 100\n",
    "    if (new <= baseline):\n",
    "        return ((baseline / new) - 1)\n",
    "    else:\n",
    "        return (-(new / baseline) + 1)\n",
    "\n",
    "def bar_plot(data, query, ax, col):\n",
    "    data = data[data['query'] == query]\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    methods = data['method'].values[1:] # drop the first method which is the single-table\n",
    "    times = data['time'].values\n",
    "    improvement_factors = [ compute_improvement_factor(times[0], t) for t in times[1:] ]\n",
    "    x = np.arange(len(methods[1:]))  # the label locations\n",
    "    min_ylim = -1\n",
    "    max_ylim = 1\n",
    "    ax.set_ylim(min_ylim, max_ylim)\n",
    "    width = 0.01  # the width of the bars\n",
    "    multiplier = 0\n",
    "    for method, improv_factor in zip(methods, improvement_factors):\n",
    "        offset = width * multiplier\n",
    "        bars = ax.bar(offset, improv_factor, width, color=colormap[method], label=method)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height < min_ylim:\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2, 0, f'{round(height, 2)}', ha='center', va='bottom', fontsize=8)\n",
    "            elif height > max_ylim:\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2, -0.2, f'{round(height, 2)}', ha='center', va='bottom', fontsize=8)\n",
    "        multiplier += 1\n",
    "    ax.set_xticks([])\n",
    "    if col != 0:\n",
    "        pass\n",
    "    else:\n",
    "        ax.set_ylabel(\"Improvement factor\")\n",
    "        # ax.set_yticks([-100, -50, 0, 50, 100])\n",
    "\n",
    "    ax.set_xlabel(query)\n",
    "    ax.axhline(0, color='black', linewidth=0.8)  # Add a horizontal line at y=0\n",
    "\n",
    "data = pd.read_csv('../rewrite-methods/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "data = data[data['data_transfer'] == True]\n",
    "methods = data['method'].unique()[1:]\n",
    "\n",
    "exclude_queries = [\n",
    "    'q13a',\n",
    "]\n",
    "data = data[~data['query'].isin(exclude_queries)]\n",
    "\n",
    "queries = natsorted(data['query'].unique())\n",
    "\n",
    "# create plot\n",
    "rows = 1\n",
    "cols = len(queries)\n",
    "fig, axes = plt.subplots(rows, cols, sharey=True, figsize=(10, 1.5))\n",
    "\n",
    "cur_row = 0\n",
    "cur_col = 0\n",
    "for q in queries:\n",
    "    if (rows == 1):\n",
    "        bar_plot(data, q, axes[cur_col], cur_col)\n",
    "    else:\n",
    "        bar_plot(data, q, axes[cur_row, cur_col], cur_col)\n",
    "    cur_col = 0 if cur_col == (cols - 1) else cur_col + 1\n",
    "    cur_row = cur_row + 1 if cur_col == 0 else cur_row\n",
    "## remove all subplots that do not contain data (in case we have an \"uncomplete\" row)\n",
    "for ax in axes.flat: # access each axes object via axs.flat\n",
    "    ## check if something was plotted \n",
    "    if not bool(ax.has_data()):\n",
    "        fig.delaxes(ax) ## delete if nothing is plotted in the axes obj\n",
    "\n",
    "# legend\n",
    "if (rows == 1):\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "else:\n",
    "    handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncols=len(methods), bbox_to_anchor=(0.5, 1.1))\n",
    "\n",
    "fig.savefig('job-rewrite-methods-improvement_factor.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3886b73-fdfe-48d9-b63f-2c3eb6a14507",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1e1a9-5d79-49b7-ad7d-b85ac469264f",
   "metadata": {},
   "source": [
    "### Varying number of joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05892213-60fc-4cd5-83ba-37a9f5c95db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # construct the following dictionary:\n",
    "    # {\n",
    "    #   'RM0': [q0_time, q1_time, ..., qn-1_time]\n",
    "    #   'RM1': [q0_time, q1_time, ..., qn-1_time]\n",
    "    # }\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    method_times = defaultdict(list)\n",
    "    for _, row in data.iterrows():\n",
    "        method_times[row['method']].append(row['time'])\n",
    "    return method_times\n",
    "\n",
    "def bar_plot(data, data_transfer, filename):\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    method_times = preprocess_data(data[data['data_transfer'] == data_transfer])\n",
    "    x = np.arange(len(queries))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    multiplier = 0\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "    for method, times in method_times.items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, times, width, color=colormap[method], label=method)\n",
    "        # ax.bar_label(rects, padding=3, size=8)\n",
    "        multiplier += 1\n",
    "\n",
    "    ax.set_ylabel('Query Execution Time [ms] (log-scale)')\n",
    "    title = f'JOB Rewrite Method Runtime w/{\"o\" if not data_transfer else \"\"} data transfer in PostgreSQL'\n",
    "    # ax.set_title(title)\n",
    "    number_of_joins = [ q.rsplit('_', 1)[1] for q in queries ]\n",
    "    ax.set_xticks(x + width + width/2, number_of_joins)\n",
    "    ax.set_xlabel('number of joins')\n",
    "    # ax.legend()\n",
    "    # ax.set_ylim(0)\n",
    "    ax.set_yscale('log')\n",
    "    # fig.legend()\n",
    "    fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "    \n",
    "    fig.savefig(filename)\n",
    "\n",
    "data = pd.read_csv('../star-schema/rewrite-results.csv')\n",
    "data = data[data['query'].str.contains('star_joins_')]\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "\n",
    "method_times = preprocess_data(data)\n",
    "single_table = method_times['0. Single Table']\n",
    "rdb_with_post_join = method_times['RM1. Dynamic SELECT DISTINCT']\n",
    "\n",
    "# for s, r in zip(single_table, rdb_with_post_join):\n",
    "#     display(compute_improvement_factor(s,r))\n",
    "\n",
    "exclude_methods = [\n",
    "    '0. Single Table'\n",
    "]\n",
    "data = data[~data['method'].isin(exclude_methods)]\n",
    "\n",
    "bar_plot(data, True, 'synthetic-rewrite-methods-joins.pdf')\n",
    "# bar_plot(data, False, 'job-rewrite-methods_no-data-transfer.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c7b7e-b4d4-4590-87b9-321424c260bc",
   "metadata": {},
   "source": [
    "### Varying number of projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67df64-cdf7-4cec-8a53-3e60378a53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_names = {\n",
    "    '2_dim': '2x dim',\n",
    "    '3_dim': '3x dim',\n",
    "    '4_dim': '4x dim',\n",
    "    'fact_1_dim': 'fact + 1x dim',\n",
    "    'fact_2_dim': 'fact + 2x dim',\n",
    "    'fact_3_dim': 'fact + 3x dim',\n",
    "    'fact_4_dim': 'fact + 4x dim'\n",
    "}\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # construct the following dictionary:\n",
    "    # {\n",
    "    #   'RM0': [q0_time, q1_time, ..., qn-1_time]\n",
    "    #   'RM1': [q0_time, q1_time, ..., qn-1_time]\n",
    "    # }\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    method_times = defaultdict(list)\n",
    "    for _, row in data.iterrows():\n",
    "        method_times[row['method']].append(row['time'])\n",
    "    return method_times\n",
    "\n",
    "def bar_plot(data, data_transfer, filename):\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    method_times = preprocess_data(data[data['data_transfer'] == data_transfer])\n",
    "    x = np.arange(len(queries))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    multiplier = 0\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "    for method, times in method_times.items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, times, width, color=colormap[method], label=method)\n",
    "        # ax.bar_label(rects, padding=3, size=8)\n",
    "        multiplier += 1\n",
    "\n",
    "    ax.set_ylabel('Query Execution Time [ms] (log-scale)')\n",
    "    title = f'JOB Rewrite Method Runtime w/{\"o\" if not data_transfer else \"\"} data transfer in PostgreSQL'\n",
    "    # ax.set_title(title)\n",
    "    names = [ query_names[q.split('_', 2)[2]] for q in queries ]\n",
    "    ax.set_xticks(x + width + width/2, names, rotation=45)\n",
    "    ax.set_xlabel('projected attributes')\n",
    "    # ax.legend()\n",
    "    # ax.set_ylim(0)\n",
    "    ax.set_yscale('log')\n",
    "    # fig.legend()\n",
    "    fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "    \n",
    "    fig.savefig(filename)\n",
    "\n",
    "data = pd.read_csv('../star-schema/rewrite-results.csv')\n",
    "data = data[data['query'].str.contains('star_proj_')]\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "\n",
    "exclude_methods = [\n",
    "    '0. Single Table'\n",
    "]\n",
    "data = data[~data['method'].isin(exclude_methods)]\n",
    "\n",
    "bar_plot(data, True, 'synthetic-rewrite-methods-projections.pdf')\n",
    "# bar_plot(data, False, 'job-rewrite-methods_no-data-transfer.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c6d89-0639-49f9-b1a1-a47313ce8e82",
   "metadata": {},
   "source": [
    "### Varying selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7b3fe-625c-4ff2-9fbd-dffad524f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # construct the following dictionary:\n",
    "    # {\n",
    "    #   'RM0': [q0_time, q1_time, ..., qn-1_time]\n",
    "    #   'RM1': [q0_time, q1_time, ..., qn-1_time]\n",
    "    # }\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    method_times = defaultdict(list)\n",
    "    for _, row in data.iterrows():\n",
    "        method_times[row['method']].append(row['time'])\n",
    "    return method_times\n",
    "\n",
    "def bar_plot(data, data_transfer, filename):\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    selectivities = [ int(q.rsplit('_', 1)[1]) / 100 for q in data['query'].unique() ]\n",
    "    method_times = preprocess_data(data[data['data_transfer'] == data_transfer])\n",
    "    x = np.arange(len(selectivities))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    multiplier = 0\n",
    "    fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.26), layout='constrained')\n",
    "    for method, times in method_times.items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, times, width, color=colormap[method], label=method)\n",
    "        # ax.bar_label(rects, padding=3, size=8)\n",
    "        multiplier += 1\n",
    "    selectivities = [ int(q.rsplit('_', 1)[1]) / 100 for q in queries ]\n",
    "    ax.set_xticks(x + width + width/2, selectivities)\n",
    "    ax.set_xlabel('Selectivity')\n",
    "    ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "    ax.legend()\n",
    "\n",
    "    log_scale = True\n",
    "    if log_scale:\n",
    "        ax.set_ylabel('Query Execution Time [ms] (log-scale)')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_ylabel('Query Execution Time [ms]')\n",
    "        ax.set_ylim(0)\n",
    "    \n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "data = pd.read_csv('../star-schema/rewrite-results.csv')\n",
    "data = data[data['query'].str.contains('star_sel_')]\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "\n",
    "exclude_methods = [\n",
    "    '0. Single Table'\n",
    "]\n",
    "data = data[~data['method'].isin(exclude_methods)]\n",
    "\n",
    "# bar_plot(data, True, 'synthetic-rewrite-methods-selectivity.pdf')\n",
    "bar_plot(data, False, 'synthetic-rewrite-methods-selectivity.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03547c1-1ee1-486a-928e-4f8de31e4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(data):\n",
    "#     # construct the following dictionary:\n",
    "#     # {\n",
    "#     #   'RM0': [q0_time, q1_time, ..., qn-1_time]\n",
    "#     #   'RM1': [q0_time, q1_time, ..., qn-1_time]\n",
    "#     # }\n",
    "#     data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "#     method_times = defaultdict(list)\n",
    "#     for _, row in data.iterrows():\n",
    "#         method_times[row['method']].append(row['time'])\n",
    "#     return method_times\n",
    "\n",
    "# data = pd.read_csv('../star-schema/rewrite-results.csv')\n",
    "# data = data[data['query'].str.contains('star_sel_')]\n",
    "# data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "# data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "# data = data.drop(['database', 'system'], axis=1)\n",
    "# exclude_methods = [\n",
    "#     '0. Single Table'\n",
    "# ]\n",
    "# data = data[~data['method'].isin(exclude_methods)]\n",
    "# data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "\n",
    "# selectivities = [ int(q.rsplit('_', 1)[1]) / 100 for q in data['query'].unique() ]\n",
    "\n",
    "# method_times = preprocess_data(data)\n",
    "# fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "# for method, times in method_times.items():\n",
    "#     ax.plot(selectivities, times, color=colormap[method], label=method)\n",
    "\n",
    "\n",
    "# ax.set_ylabel('Query Execution Time [ms]')\n",
    "# ax.set_xlabel('selectivity on all dimension tables')\n",
    "# ax.legend()\n",
    "# # ax.set_ylim(0)\n",
    "# # ax.set_yscale('log')\n",
    "# # fig.legend()\n",
    "# # fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "# fig.savefig('synthetic-rewrite-methods-selectivities.pdf', bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaaaedf-d19f-4860-8856-3ae768e1989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overhead(baseline, new):\n",
    "    assert baseline != 0, \"baseline must not be zero\"\n",
    "    if (new <= baseline): # new is faster -> improvement\n",
    "        return -((baseline - new) / baseline) * 100\n",
    "    else: # new is slower -> overhead \n",
    "        return ((new - baseline) / baseline) * 100\n",
    "        \n",
    "data = pd.read_csv('../star-schema/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "data = data[data['query'].str.contains('star_sel_')]\n",
    "data = data[data['data_transfer'] == True]\n",
    "\n",
    "single_table = data[data['method'] == '0. Single Table']\n",
    "rewrite_methods = data[data['method'] != '0. Single Table']\n",
    "\n",
    "# NOTE: be careful as ties are included (handle them manually)\n",
    "min_values = rewrite_methods.groupby(by=['query'])['time'].transform('min')\n",
    "best_rewrite_method = rewrite_methods[rewrite_methods['time'] == min_values]\n",
    "\n",
    "# ensure that both dataframes are sorted correctly\n",
    "single_table = single_table.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "best_rewrite_method = best_rewrite_method.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "\n",
    "display(single_table)\n",
    "display(best_rewrite_method)\n",
    "\n",
    "assert len(single_table) == len(best_rewrite_method)\n",
    "\n",
    "for (index1, st), (index2, rm) in zip(single_table.iterrows(), best_rewrite_method.iterrows()):\n",
    "    print(f\"{st['query']}: {round(overhead(int(st['time']), int(rm['time'])), 1)}\")\n",
    "    # print(f\"{st['query']}: {round(int(st['time']) / int(rm['time']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352c924-b13d-460c-a341-ddc2900dfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_improvement_factor(baseline, new):\n",
    "    \"\"\"\n",
    "    Compute the improvement factor of a new method against a baseline.\n",
    "\n",
    "    Args:\n",
    "    baseline (float): Performance metric of the baseline method.\n",
    "    new (float): Performance metric of the new method.\n",
    "\n",
    "    Returns:\n",
    "    float: Improvement factor as a percentage.\n",
    "    \"\"\"\n",
    "    assert baseline != 0, \"baseline must not be zero\"\n",
    "    # return ((baseline - new) / baseline) * 100\n",
    "    if (new <= baseline):\n",
    "        return ((baseline / new) - 1)\n",
    "    else:\n",
    "        return (-(new / baseline) + 1)\n",
    "\n",
    "def bar_plot(data, query, ax, col):\n",
    "    data = data[data['query'] == query]\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    methods = data['method'].values[1:] # drop the first method which is the single-table\n",
    "    times = data['time'].values\n",
    "    improvement_factors = [ compute_improvement_factor(times[0], t) for t in times[1:] ]\n",
    "    x = np.arange(len(methods[1:]))  # the label locations\n",
    "    min_ylim = -7\n",
    "    max_ylim = 7\n",
    "    ax.set_ylim(min_ylim, max_ylim)\n",
    "    width = 0.01  # the width of the bars\n",
    "    multiplier = 0\n",
    "    for method, improv_factor in zip(methods, improvement_factors):\n",
    "        offset = width * multiplier\n",
    "        bars = ax.bar(offset, improv_factor, width, color=colormap[method], label=method)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height < min_ylim:\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2, 0, f'{round(height, 1)}', ha='center', va='bottom', fontsize=8)\n",
    "            elif height > max_ylim:\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2, -0.2, f'{round(height, 1)}', ha='center', va='bottom', fontsize=8)\n",
    "        multiplier += 1\n",
    "    ax.set_xticks([])\n",
    "    if col != 0:\n",
    "        pass\n",
    "    else:\n",
    "        ax.set_ylabel(\"Improvement factor (\\%)\")\n",
    "        # ax.set_yticks([-800, -400, 0, 400, 800])\n",
    "\n",
    "    ax.axhline(0, color='black', linewidth=0.8)  # Add a horizontal line at y=0\n",
    "    ax.set_xlabel(f\"{int(query.rsplit('_', 1)[1]) / 100}\")\n",
    "\n",
    "\n",
    "data = pd.read_csv('../star-schema/rewrite-results.csv')\n",
    "data = data[data['query'].str.contains('star_sel_')]\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "data = data[data['data_transfer'] == True]\n",
    "methods = data['method'].unique()[1:]\n",
    "queries = natsorted(data['query'].unique())\n",
    "\n",
    "# create plot\n",
    "rows = 1\n",
    "cols = len(queries)\n",
    "fig, axes = plt.subplots(rows, cols, sharey=True, figsize=(10, 1.5))\n",
    "\n",
    "cur_row = 0\n",
    "cur_col = 0\n",
    "for q in queries:\n",
    "    if (rows == 1):\n",
    "        bar_plot(data, q, axes[cur_col], cur_col)\n",
    "    else:\n",
    "        bar_plot(data, q, axes[cur_row, cur_col], cur_col)\n",
    "    cur_col = 0 if cur_col == (cols - 1) else cur_col + 1\n",
    "    cur_row = cur_row + 1 if cur_col == 0 else cur_row\n",
    "## remove all subplots that do not contain data (in case we have an \"uncomplete\" row)\n",
    "for ax in axes.flat: # access each axes object via axs.flat\n",
    "    ## check if something was plotted \n",
    "    if not bool(ax.has_data()):\n",
    "        fig.delaxes(ax) ## delete if nothing is plotted in the axes obj\n",
    "\n",
    "# legend\n",
    "if (rows == 1):\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "else:\n",
    "    handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncols=len(methods), bbox_to_anchor=(0.5, 1.1))\n",
    "# Add a common x-axis label for all subplots\n",
    "fig.text(0.5, -0.1, 'Selectivity', ha='center', va='center')\n",
    "\n",
    "fig.savefig('synthetic-rewrite-methods-improvement_factor.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ff91c-8c3f-4fef-9cec-88d3a3f7de8d",
   "metadata": {},
   "source": [
    "# RESULTDB Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db905c6-1618-4ad1-8105-d7de161babb4",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c7bfc-e4b7-4bfb-ab79-6c534ea758e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the following dictionary:\n",
    "# {\n",
    "#   'Single-Table': [q4_time, q5_time, ...]\n",
    "#   'Single-Table + Denom': [q4_time, q5_time, ...]\n",
    "#   'Result-DB': [q4_time, q5_time, ...]\n",
    "# }\n",
    "algorithm_times = {\n",
    "    'Single-Table': [],\n",
    "    'Single-Table + Decompose': [],\n",
    "    'Result-DB': []\n",
    "}\n",
    "# ensure that `queries` is sorted!\n",
    "queries = [\n",
    "    'q1a',\n",
    "    'q3b',\n",
    "    'q4a',\n",
    "    'q5b',\n",
    "    'q7b'\n",
    "]\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            algorithm_times['Single-Table'].append(execution_time)\n",
    "        elif \"decompose\" in algorithm:\n",
    "            algorithm_times['Single-Table + Decompose'].append(execution_time)\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            algorithm_times['Result-DB'].append(execution_time)\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "# display(algorithm_times)\n",
    "            \n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.10  # the width of the bars\n",
    "multiplier = 0\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "for algorithm, time in algorithm_times.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, time, width, label=algorithm)\n",
    "    # ax.bar_label(rects, padding=3, size=8)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_ylabel('Runtime [ms]')\n",
    "ax.set_title('JOB Algorithm Runtime in mutable')\n",
    "ax.set_xticks(x + width, processed_queries)\n",
    "ax.legend()\n",
    "ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "# fig.legend()\n",
    "# fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "fig.savefig('job-benchmarks.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b057539-dad4-45b3-b9e6-f24a071e213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the following dictionary:\n",
    "# {\n",
    "#   'Single-Table': [q4_time, q5_time, ...]\n",
    "#   'Single-Table + Decompose': [q4_time, q5_time, ...]\n",
    "#   'Result-DB': [q4_time, q5_time, ...]\n",
    "# }\n",
    "# ensure that `queries` is sorted!\n",
    "queries = [\n",
    "    'q1a',\n",
    "    'q3b',\n",
    "    'q4a',\n",
    "    'q5b',\n",
    "    # 'q7b'\n",
    "]\n",
    "\n",
    "single_table_decompose = []\n",
    "resultdb = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            single_table_execution_time = execution_time\n",
    "        elif \"decompose\" in algorithm:\n",
    "            decompose_execution_time = execution_time\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            resultdb_execution_time = execution_time\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "            \n",
    "    single_table_decompose.append((single_table_execution_time, decompose_execution_time - single_table_execution_time))\n",
    "    resultdb.append(resultdb_execution_time)\n",
    "\n",
    "assert len(single_table_decompose) == len(resultdb) == len(processed_queries), f'number of measurements has to match the processed queries'\n",
    "\n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.30  # the width of the bars\n",
    "fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.20), layout='constrained')\n",
    "\n",
    "st_time = [ st_decomp[0] for st_decomp in single_table_decompose ]\n",
    "decomp_time = [ st_decomp[1] for st_decomp in single_table_decompose ]\n",
    "ax.bar(x, st_time, width, color=colormap['Single Table'], edgecolor='black', label='Single Table')\n",
    "ax.bar(x, decomp_time, width, bottom=st_time, color=colormap['Decompose'], edgecolor='black', label='Decompose')\n",
    "\n",
    "offset = width\n",
    "ax.bar(x + offset, resultdb, width, color=colormap['Result DB'], edgecolor='black', label='Result DB')\n",
    "\n",
    "ax.set_ylabel('Query Execution Time [ms]')\n",
    "# ax.set_title('JOB Algorithm Runtime in mutable')\n",
    "ax.set_xlabel('JOB Queries')\n",
    "ax.set_xticks(x + width / 2, processed_queries)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0)\n",
    "ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "# ax.set_yscale('log')\n",
    "# fig.legend()\n",
    "# fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "# legend\n",
    "\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc='upper center', ncols=len(methods), bbox_to_anchor=(0.5, 1.25))\n",
    "\n",
    "fig.savefig('job-algorithm.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a67ed-4bfd-49ce-ae52-ce125adb8f61",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba59f46-d967-4047-8a12-e041000ca184",
   "metadata": {},
   "source": [
    "### Varying number of joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af42db-aab6-4751-9ae0-4dcd4c17092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the following dictionary:\n",
    "# {\n",
    "#   'Single-Table': [q4_time, q5_time, ...]\n",
    "#   'Single-Table + Decompose': [q4_time, q5_time, ...]\n",
    "#   'Result-DB': [q4_time, q5_time, ...]\n",
    "# }\n",
    "algorithm_times = {\n",
    "    'Single Table': [],\n",
    "    'Single Table + Decompose': [],\n",
    "    'Result DB': []\n",
    "}\n",
    "# ensure that `queries` is sorted!\n",
    "queries = [\n",
    "    'star_joins_1',\n",
    "    'star_joins_2',\n",
    "    'star_joins_3',\n",
    "    'star_joins_4'\n",
    "]\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../star-schema/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query or experiment[0] in query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            algorithm_times['Single Table'].append(execution_time)\n",
    "        elif \"decompose\" in algorithm:\n",
    "            algorithm_times['Single Table + Decompose'].append(execution_time)\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            algorithm_times['Result DB'].append(execution_time)\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "\n",
    "    assert len(algorithm_times['Single Table']) == len(algorithm_times['Single Table + Decompose']) == len(algorithm_times['Result DB']), f'each algorithm must have the same number of measurements'\n",
    "            \n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.10  # the width of the bars\n",
    "multiplier = 0\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "for algorithm, time in algorithm_times.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, time, width, label=algorithm)\n",
    "    # ax.bar_label(rects, padding=3, size=8)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_ylabel('Runtime [ms]')\n",
    "ax.set_title('Synthetic Algorithm Runtime in mutable')\n",
    "ax.set_xticks(x + width, processed_queries)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "# fig.legend()\n",
    "# fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "fig.savefig('synthetic-benchmarks.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90828c67-9d16-4b2e-9c44-7bce85070567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that `queries` is sorted!\n",
    "queries = [\n",
    "    'star_joins_1',\n",
    "    'star_joins_2',\n",
    "    'star_joins_3',\n",
    "    'star_joins_4'\n",
    "]\n",
    "single_table_decompose = []\n",
    "resultdb = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../star-schema/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query or experiment[0] in query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            single_table_execution_time = execution_time\n",
    "        elif \"decompose\" in algorithm:\n",
    "            decompose_execution_time = execution_time\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            resultdb_execution_time = execution_time\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "            \n",
    "    single_table_decompose.append((single_table_execution_time, decompose_execution_time - single_table_execution_time))\n",
    "    resultdb.append(resultdb_execution_time)\n",
    "\n",
    "assert len(single_table_decompose) == len(resultdb) == len(processed_queries), f'number of measurements has to match the processed queries'\n",
    "\n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.10  # the width of the bars\n",
    "multiplier = 0\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "\n",
    "st_time = [ st_decomp[0] for st_decomp in single_table_decompose ]\n",
    "decomp_time = [ st_decomp[1] for st_decomp in single_table_decompose ]\n",
    "ax.bar(x, st_time, width, color=colormap['Single Table'], edgecolor='black', label='Single Table')\n",
    "ax.bar(x, decomp_time, width, bottom=st_time, color=colormap['Decompose'], edgecolor='black', label='Decompose')\n",
    "\n",
    "offset = width\n",
    "ax.bar(x + offset, resultdb, width, color=colormap['Result DB'], edgecolor='black', label='Result DB')\n",
    "\n",
    "ax.set_ylabel('Runtime [ms]')\n",
    "ax.set_title('Synthetic Algorithm Runtime in mutable')\n",
    "ax.set_xticks(x + width / 2, processed_queries)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "# fig.legend()\n",
    "# fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "fig.savefig('synthetic-benchmarks.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e95b5-fbca-47df-8b55-e0170c4a5854",
   "metadata": {},
   "source": [
    "### Varying projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfc497-91f5-48af-9644-b46218986a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that `queries` is sorted!\n",
    "queries = [\n",
    "    'star_proj_2_dim',\n",
    "    'star_proj_3_dim',\n",
    "    'star_proj_4_dim',\n",
    "    'star_proj_fact_1_dim',\n",
    "    'star_proj_fact_2_dim',\n",
    "    'star_proj_fact_3_dim',\n",
    "    'star_proj_fact_4_dim',\n",
    "]\n",
    "single_table_decompose = []\n",
    "resultdb = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../star-schema/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query or experiment[0] in query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            single_table_execution_time = execution_time\n",
    "        elif \"decompose\" in algorithm:\n",
    "            decompose_execution_time = execution_time\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            resultdb_execution_time = execution_time\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "            \n",
    "    single_table_decompose.append((single_table_execution_time, decompose_execution_time - single_table_execution_time))\n",
    "    resultdb.append(resultdb_execution_time)\n",
    "\n",
    "assert len(single_table_decompose) == len(resultdb) == len(processed_queries), f'number of measurements has to match the processed queries'\n",
    "\n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.10  # the width of the bars\n",
    "multiplier = 0\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=(4,3))\n",
    "\n",
    "st_time = [ st_decomp[0] for st_decomp in single_table_decompose ]\n",
    "decomp_time = [ st_decomp[1] for st_decomp in single_table_decompose ]\n",
    "ax.bar(x, st_time, width, color=colormap['Single Table'], edgecolor='black', label='Single Table')\n",
    "ax.bar(x, decomp_time, width, bottom=st_time, color=colormap['Decompose'], edgecolor='black', label='Decompose')\n",
    "\n",
    "offset = width\n",
    "ax.bar(x + offset, resultdb, width, color=colormap['Result DB'], edgecolor='black', label='Result DB')\n",
    "\n",
    "ax.set_ylabel('Runtime [ms]')\n",
    "ax.set_title('Synthetic Algorithm Runtime in mutable')\n",
    "ax.set_xticks(x + width / 2, processed_queries, rotation=45)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0)\n",
    "# ax.set_yscale('log')\n",
    "# fig.legend()\n",
    "# fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "fig.savefig('synthetic-benchmarks.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a391cf-d11f-429f-9864-a35820f49b0f",
   "metadata": {},
   "source": [
    "### Varying selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045313bf-cbab-465b-a53e-ae5f02339ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that `queries` is sorted!\n",
    "queries = [\n",
    "    'star_sel_10',\n",
    "    'star_sel_20',\n",
    "    'star_sel_30',\n",
    "    'star_sel_40',\n",
    "    'star_sel_50',\n",
    "    'star_sel_60',\n",
    "    'star_sel_70',\n",
    "    'star_sel_80',\n",
    "    'star_sel_90',\n",
    "    'star_sel_100',\n",
    "]\n",
    "single_table_decompose = []\n",
    "resultdb = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../star-schema/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query or experiment[0] in query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            single_table_execution_time = execution_time\n",
    "        elif \"decompose\" in algorithm:\n",
    "            decompose_execution_time = execution_time\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            resultdb_execution_time = execution_time\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "            \n",
    "    single_table_decompose.append((single_table_execution_time, decompose_execution_time - single_table_execution_time))\n",
    "    resultdb.append(resultdb_execution_time)\n",
    "\n",
    "assert len(single_table_decompose) == len(resultdb) == len(processed_queries), f'number of measurements has to match the processed queries'\n",
    "\n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.30  # the width of the bars\n",
    "multiplier = 0\n",
    "fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.20), layout='constrained')\n",
    "\n",
    "st_time = [ st_decomp[0] for st_decomp in single_table_decompose ]\n",
    "decomp_time = [ st_decomp[1] for st_decomp in single_table_decompose ]\n",
    "ax.bar(x, st_time, width, color=colormap['Single Table'], edgecolor='black', label='Single Table')\n",
    "ax.bar(x, decomp_time, width, bottom=st_time, color=colormap['Decompose'], edgecolor='black', label='Decompose')\n",
    "\n",
    "offset = width\n",
    "ax.bar(x + offset, resultdb, width, color=colormap['Result DB'], edgecolor='black', label='Result DB')\n",
    "\n",
    "ax.set_ylabel('Query Execution Time [ms]')\n",
    "# ax.set_title('Synthetic Algorithm Runtime in mutable')\n",
    "selectivities = [ int(q.rsplit('_', 1)[1]) / 100 for q in processed_queries ]\n",
    "ax.set_xticks(x + width / 2, selectivities)\n",
    "ax.set_xlabel('Selectivity')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0)\n",
    "ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "# ax.set_yscale('log')\n",
    "# fig.legend()\n",
    "# fig.legend(ncols=int(len(method_times)/1.5), bbox_to_anchor=(0.5, 1), loc='lower center')\n",
    "\n",
    "fig.savefig('synthetic-algorithm.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
